from llm import *
from util.request import get_yesterday_arxiv_papers
from util.construct_email import *
from tqdm import tqdm
import json
import os
from datetime import datetime, timezone
import time
import random
import smtplib
from email.header import Header
from email.utils import parseaddr, formataddr
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import requests
from rank_bm25 import BM25Okapi
from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForMaskedLM
import torch



class ArxivDaily:
    def __init__(
        self,
        categories: list[str],
        max_entries: int,
        max_paper_num: int,
        provider: str,
        model: str,
        base_url: str,
        api_key: str,
        description: str,
        num_workers: int,
        temperature: float,
        save_dir: str,
        filter_method: str = "bm25",  
        language: str = "korean",     
    ):
        self.model_name = model
        self.base_url = base_url
        self.api_key = api_key
        self.max_paper_num = max_paper_num
        self.save_dir = save_dir
        self.num_workers = num_workers
        self.temperature = temperature
        self.filter_method = filter_method.lower()  
        self.language = language.lower()            

        self.run_datetime = datetime.now(timezone.utc)
        self.run_date = self.run_datetime.strftime("%Y-%m-%d")
        base_dir = os.path.dirname(os.path.abspath(__file__))
        self.cache_dir = os.path.join(base_dir, save_dir, self.run_date,"json")
        os.makedirs(self.cache_dir, exist_ok=True)
        self.papers = {}
        for category in categories:
            self.papers[category] = get_yesterday_arxiv_papers(category, max_entries)
            print(
                "{} papers on arXiv for {} are fetched.".format(
                    len(self.papers[category]), category
                )
            )
            # avoid being blocked
            sleep_time = random.randint(5, 15)
            time.sleep(sleep_time)

        provider = provider.lower()
        if provider == "ollama":
            self.model = Ollama(model)
        elif provider == "openai" or provider == "siliconflow":
            self.model = GPT(model, base_url, api_key)
        else:
            assert False, "Model not supported."
        print(
            "Model initialized successfully. Using {} provided by {}.".format(
                model, provider
            )
        )

        self.description = description
        self.lock = threading.Lock()  # æ·»åŠ çº¿ç¨‹é”

    def get_response(self, title, abstract):
        if self.language == "english":
            prompt = f"""
            You are a helpful academic research assistant.
            Below is a description of my current research area:
            {self.description}

            Here is a paper retrieved from arXiv yesterday.
            Title: {title}
            Abstract: {abstract}

            Please do the following:
            1. Summarize the main content of this paper in English (max 3 sentences).
            2. Evaluate how relevant it is to my research field (0â€“10, where 0 = not related, 10 = highly relevant).

            Return exactly in the following JSON format:
            {{
                "summary": "<summary>",
                "relevance": <score>
            }}

            âš ï¸ Important:
            - Use English only.
            - Return JSON only, with no explanations or code blocks.
            """
        
        elif self.language == "chinese":
            prompt = """
                ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„å­¦æœ¯ç ”ç©¶åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©æˆ‘æ„å»ºæ¯æ—¥è®ºæ–‡æ¨èç³»ç»Ÿã€‚
                ä»¥ä¸‹æ˜¯æˆ‘æœ€è¿‘ç ”ç©¶é¢†åŸŸçš„æè¿°ï¼š
                {}
            """.format(self.description)
            prompt += """
                ä»¥ä¸‹æ˜¯æˆ‘ä»æ˜¨å¤©çš„ arXiv çˆ¬å–çš„è®ºæ–‡ï¼Œæˆ‘ä¸ºä½ æä¾›äº†æ ‡é¢˜å’Œæ‘˜è¦ï¼š
                æ ‡é¢˜: {}
                æ‘˜è¦: {}
            """.format(title, abstract)
            prompt += """
                1. æ€»ç»“è¿™ç¯‡è®ºæ–‡çš„ä¸»è¦å†…å®¹ã€‚
                2. è¯·è¯„ä¼°è¿™ç¯‡è®ºæ–‡ä¸æˆ‘ç ”ç©¶é¢†åŸŸçš„ç›¸å…³æ€§ï¼Œå¹¶ç»™å‡º 0-10 çš„è¯„åˆ†ã€‚å…¶ä¸­ 0 è¡¨ç¤ºå®Œå…¨ä¸ç›¸å…³ï¼Œ10 è¡¨ç¤ºé«˜åº¦ç›¸å…³ã€‚
                
                è¯·æŒ‰ä»¥ä¸‹ JSON æ ¼å¼ç»™å‡ºä½ çš„å›ç­”ï¼š
                {
                    "summary": <ä½ çš„æ€»ç»“>,
                    "relevance": <ä½ çš„è¯„åˆ†>
                }
                ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚
                ç›´æ¥è¿”å›ä¸Šè¿° JSON æ ¼å¼ï¼Œæ— éœ€ä»»ä½•é¢å¤–è§£é‡Šã€‚
            """

        else:  # default = korean
            prompt = f"""
            ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” í•™ë¬¸ ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
            ì•„ë˜ëŠ” ë‚´ê°€ ì—°êµ¬ ì¤‘ì¸ ë¶„ì•¼ ì„¤ëª…ì…ë‹ˆë‹¤:
            {self.description}

            ë‹¤ìŒì€ ì–´ì œ arXivì—ì„œ ìˆ˜ì§‘í•œ ë…¼ë¬¸ì…ë‹ˆë‹¤.
            ì œëª©: {title}
            ì´ˆë¡: {abstract}

            ì•„ë˜ ë‘ ê°€ì§€ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”:
            1. ì´ ë…¼ë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”. (ìµœëŒ€ 3ë¬¸ì¥)
            2. ë‚´ ì—°êµ¬ ë¶„ì•¼ì™€ì˜ ê´€ë ¨ë„ë¥¼ 0~10ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”. (0 = ì „í˜€ ë¬´ê´€, 10 = ë§¤ìš° ê´€ë ¨)

            ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì¶œë ¥í•˜ì„¸ìš”:
            {{
                "summary": "<ìš”ì•½ë¬¸>",
                "relevance": <ì ìˆ˜>
            }}

            âš ï¸ì£¼ì˜:
            - ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
            - JSON ì™¸ì˜ ë¶ˆí•„ìš”í•œ ë¬¸ì¥ì´ë‚˜ ì½”ë“œë¸”ë¡(```json ë“±) ì—†ì´ ê²°ê³¼ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
            """

        response = self.model.inference(prompt, temperature=self.temperature)
        return response


    def bm25_filter(self, papers, description, top_k=50):
        corpus = [(p["title"] + " " + p["abstract"]).lower().split() for p in papers]
        bm25 = BM25Okapi(corpus)
        query = description.lower().split()
        scores = bm25.get_scores(query)
        ranked_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
        filtered = [papers[i] for i in ranked_idx]
        for i, paper in enumerate(filtered):
            paper["bm25_score"] = scores[ranked_idx[i]]
        return filtered


    def dpr_filter(self, papers, description, top_k=50):
        device = "cuda" if torch.cuda.is_available() else "cpu"

        if not hasattr(self, "_dpr_cache"):
            print(f"[DPR] Loading models on {device} ...")
            self._dpr_cache = {
                "q_tok": DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base"),
                "q_enc": DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base").to(device),
                "c_tok": DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base"),
                "c_enc": DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base").to(device),
            }
            self._dpr_cache["q_enc"].eval()
            self._dpr_cache["c_enc"].eval()

        q_tok, q_enc = self._dpr_cache["q_tok"], self._dpr_cache["q_enc"]
        c_tok, c_enc = self._dpr_cache["c_tok"], self._dpr_cache["c_enc"]

        q_inputs = q_tok(description, return_tensors="pt", truncation=True, max_length=256).to(device)
        with torch.no_grad():
            q_emb = q_enc(**q_inputs).pooler_output

        doc_texts = [p["title"] + " " + p["abstract"] for p in papers]
        doc_embs = []
        for text in doc_texts:
            d_inputs = c_tok(text, return_tensors="pt", truncation=True, max_length=256).to(device)
            with torch.no_grad():
                d_emb = c_enc(**d_inputs).pooler_output
            doc_embs.append(d_emb.cpu())

        doc_embs = torch.cat(doc_embs, dim=0)
        sims = torch.matmul(q_emb.cpu(), doc_embs.T).squeeze(0)
        top_idx = torch.topk(sims, k=min(top_k, len(sims))).indices.tolist()
        filtered = [papers[i] for i in top_idx]

        for i, p in enumerate(filtered):
            p["dpr_score"] = float(sims[top_idx[i]])
        return filtered

    
    def splade_filter(self, papers, description, top_k=50):
        device = "cuda" if torch.cuda.is_available() else "cpu"

        if not hasattr(self, "_splade_cache"):
            print(f"[Splade] Loading model on {device} ...")
            from transformers import AutoTokenizer, AutoModelForMaskedLM
            tok = AutoTokenizer.from_pretrained("naver/splade-cocondenser-ensembledistil")
            model = AutoModelForMaskedLM.from_pretrained("naver/splade-cocondenser-ensembledistil").to(device)
            model.eval()
            self._splade_cache = {"tok": tok, "model": model}

        tok, model = self._splade_cache["tok"], self._splade_cache["model"]

        def encode(text):
            inputs = tok(text, return_tensors="pt", truncation=True, max_length=256).to(device)
            with torch.no_grad():
                logits = model(**inputs).logits.squeeze(0)
            weights = torch.log(1 + torch.relu(logits))
            vec = torch.max(weights, dim=0).values
            return vec.cpu()

        q_vec = encode(description)
        doc_vecs = [encode(p["title"] + " " + p["abstract"]) for p in papers]

        sims = []
        for v in doc_vecs:
            denom = (torch.norm(q_vec) * torch.norm(v)).item()
            sims.append((torch.dot(q_vec, v) / denom).item() if denom != 0 else 0.0)

        sorted_idx = sorted(range(len(sims)), key=lambda i: sims[i], reverse=True)
        filtered = [papers[i] for i in sorted_idx[:top_k]]

        for i, p in enumerate(filtered):
            p["splade_score"] = float(sims[sorted_idx[i]])
        return filtered



    def process_paper(self, paper, max_retries=5):
        retry_count = 0
        cache_path = os.path.join(self.cache_dir, f"{paper['arXiv_id']}.json")

        # Try to load from cache if available
        if os.path.exists(cache_path):
            try:
                with open(cache_path, "r", encoding="utf-8") as cache_file:
                    cached_result = json.load(cache_file)
                print(f"âœ… Successfully loaded cache file: {cache_path}")
                return cached_result
            except (json.JSONDecodeError, OSError) as e:
                print(f"âš ï¸ Failed to load cache file {cache_path}: {e} â†’ Rebuilding new file.")

        # Try processing the paper within the retry limit
        while retry_count < max_retries:
            try:
                title = paper["title"]
                abstract = paper["abstract"]
                response = self.get_response(title, abstract)
                response = response.strip("```").strip("json")
                response = json.loads(response)

                relevance_score = float(response["relevance"])
                summary = response["summary"]

                result = {
                    "title": title,
                    "arXiv_id": paper["arXiv_id"],
                    "abstract": abstract,
                    "summary": summary,
                    "relevance_score": relevance_score,
                    "pdf_url": paper["pdf_url"],
                }

                # Save to cache
                try:
                    with self.lock:
                        with open(cache_path, "w", encoding="utf-8") as cache_file:
                            json.dump(result, cache_file, ensure_ascii=False, indent=2)
                    print(f"ğŸ’¾ Cache saved successfully: {cache_path}")
                except OSError as write_error:
                    print(f"âš ï¸ Error occurred while saving cache: {write_error}")

                return result

            except Exception as e:
                retry_count += 1
                print(f"âŒ Error while processing paper {paper['arXiv_id']}: {e}")
                print(f"â³ Retrying... ({retry_count}/{max_retries})")

                if retry_count == max_retries:
                    print(f"ğŸš« Exceeded maximum retries ({max_retries}). Skipping paper {paper['arXiv_id']}.")
                    # Return fallback result on failure
                    result = {
                        "title": paper["title"],
                        "arXiv_id": paper["arXiv_id"],
                        "abstract": paper["abstract"],
                        "summary": "Failed to summarize this paper.",
                        "relevance_score": 0.0,
                        "pdf_url": paper.get("pdf_url", ""),
                    }
                    try:
                        with self.lock:
                            with open(cache_path, "w", encoding="utf-8") as cache_file:
                                json.dump(result, cache_file, ensure_ascii=False, indent=2)
                        print(f"âš ï¸ Saved failure result to cache: {cache_path}")
                    except OSError as write_error:
                        print(f"âš ï¸ Error occurred while saving failure result: {write_error}")
                    return result

                time.sleep(1)  # Wait before retrying



    def get_recommendation(self):
        recommendations = {}

        for category, papers in self.papers.items():
            # Use the filter method chosen by the user (from --filter_method)
            if self.filter_method == "bm25":
                filtered = self.bm25_filter(papers, self.description, top_k=self.max_paper_num * 2)
            elif self.filter_method == "dpr":
                filtered = self.dpr_filter(papers, self.description, top_k=self.max_paper_num * 2)
            elif self.filter_method == "splade":
                filtered = self.splade_filter(papers, self.description, self.max_paper_num * 2)
            else:
                filtered = papers  # No filtering

            for p in filtered:
                recommendations[p["arXiv_id"]] = p

        print(f"[Filter: {self.filter_method.upper()}] Retrieved {len(recommendations)} papers after filtering.")
    
        # Proceed to LLM inference
        recommendations_ = []
        print("Performing LLM inference...")

        with ThreadPoolExecutor(self.num_workers) as executor:
            futures = []
            for arXiv_id, paper in recommendations.items():
                futures.append(executor.submit(self.process_paper, paper))
            for future in tqdm(
                as_completed(futures),
                total=len(futures),
                desc="Processing papers",
                unit="paper",
            ):
                result = future.result()
                if result:
                    recommendations_.append(result)

        # Sort by LLM relevance score
        recommendations_ = sorted(
            recommendations_, key=lambda x: x["relevance_score"], reverse=True
        )[: self.max_paper_num]

        # Save recommendation to markdown file
        current_time = self.run_datetime
        save_path = os.path.join(
            self.save_dir, self.run_date, f"{current_time.strftime('%Y-%m-%d')}.md"
        )
        with open(save_path, "w") as f:
            f.write("# Daily arXiv Papers\n")
            f.write(f"## Date: {current_time.strftime('%Y-%m-%d')}\n")
            f.write(f"## Description: {self.description}\n")
            f.write("## Papers:\n")
            for i, paper in enumerate(recommendations_):
                f.write(f"### {i + 1}. {paper['title']}\n")
                f.write(f"#### Abstract:\n")
                f.write(f"{paper['abstract']}\n")
                f.write(f"#### Summary:\n")
                f.write(f"{paper['summary']}\n")
                f.write(f"#### Relevance Score: {paper['relevance_score']}\n")
                f.write(f"#### PDF URL: {paper['pdf_url']}\n")
                f.write("\n")

        return recommendations_

    def summarize(self, recommendations):
        overview = ""
        for i in range(len(recommendations)):
            overview += f"{i + 1}. {recommendations[i]['title']} - {recommendations[i]['summary']}\n"

        # -------- Chinese Version --------
        if self.language == "chinese":

            prompt_context = """
                ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„å­¦æœ¯ç ”ç©¶åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©æˆ‘æ„å»ºæ¯æ—¥è®ºæ–‡æ¨èç³»ç»Ÿã€‚
                ä»¥ä¸‹æ˜¯æˆ‘æœ€è¿‘ç ”ç©¶é¢†åŸŸçš„æè¿°ï¼š
                {}
            """.format(self.description)
            papers_context = """
                ä»¥ä¸‹æ˜¯æˆ‘ä»æ˜¨å¤©çš„ arXiv çˆ¬å–çš„è®ºæ–‡ï¼Œæˆ‘ä¸ºä½ æä¾›äº†æ ‡é¢˜å’Œæ‘˜è¦ï¼š
                {}
            """.format(overview)
            json_instruction = """
                è¯·åŠ¡å¿…ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON ç»“æ„è¿”å›å†…å®¹ï¼Œä¸è¦æ·»åŠ é¢å¤–æ–‡æœ¬æˆ–ä»£ç å—ï¼š
                {{
                "trend_summary": "<æ€»ä½“è¶‹åŠ¿ï¼Œç”¨ä¸­æ–‡,ä½¿ç”¨ html çš„è¯­æ³•ï¼Œä¸è¦ä½¿ç”¨ markdown çš„è¯­æ³•>",
                "recommendations": [
                    {{
                    "title": "<è®ºæ–‡æ ‡é¢˜>",
                    "relevance_label": "<é«˜åº¦ç›¸å…³/ç›¸å…³/ä¸€èˆ¬ç›¸å…³>",
                    "recommend_reason": "<ä¸ºä»€ä¹ˆå€¼å¾—æˆ‘è¯»>",
                    "key_contribution": "<ä¸€å¥è¯æ¦‚æ‹¬è®ºæ–‡å…³é”®è´¡çŒ®>"
                    }}
                ],
                "additional_observation": "<è¡¥å……è§‚å¯Ÿï¼Œè‹¥æ— è¯·å†™â€˜æš‚æ— â€™>"
                }}

                ä»»åŠ¡è¦æ±‚ï¼š
                1. ç»™å‡ºä»Šå¤©è®ºæ–‡ä½“ç°çš„æ•´ä½“ç ”ç©¶è¶‹åŠ¿ï¼Œè§£é‡Šå…¶ä¸æˆ‘ç ”ç©¶å…´è¶£çš„è”ç³»ã€‚
                2. ç²¾é€‰æœ€å€¼å¾—æˆ‘ç²¾è¯»çš„è®ºæ–‡ï¼ˆå»ºè®®è¿”å› 3-5 ç¯‡ï¼Œå¯æŒ‰å®é™…æƒ…å†µå¢å‡ï¼‰ï¼Œè¯´æ˜æ¨èç†ç”±å¹¶çªå‡ºå…³é”®è´¡çŒ®ã€‚
                3. å¦‚æœ‰éœ€è¦æŒç»­å…³æ³¨æˆ–æ½œåœ¨é£é™©çš„æ–¹å‘ï¼Œè¯·åœ¨è¡¥å……è§‚å¯Ÿä¸­è¯´æ˜ï¼›è‹¥æ²¡æœ‰è¯·å†™â€œæš‚æ— â€ã€‚
            """
            html_instruction = """
                è¯·ç›´æ¥è¾“å‡ºä¸€æ®µ HTML ç‰‡æ®µï¼Œä¸¥æ ¼éµå¾ªä»¥ä¸‹ç»“æ„ï¼Œä¸è¦åŒ…å« JSONã€Markdown æˆ–å¤šä½™è¯´æ˜ï¼š
                <div class="summary-wrapper">
                <div class="summary-section">
                    <h2>ä»Šæ—¥ç ”ç©¶è¶‹åŠ¿</h2>
                    <p>...</p>
                </div>
                <div class="summary-section">
                    <h2>é‡ç‚¹æ¨è</h2>
                    <ol class="summary-list">
                    <li class="summary-item">
                        <div class="summary-item__header"><span class="summary-item__title">è®ºæ–‡æ ‡é¢˜</span><span class="summary-pill">ç›¸å…³æ€§</span></div>
                        <p><strong>æ¨èç†ç”±ï¼š</strong>...</p>
                        <p><strong>å…³é”®è´¡çŒ®ï¼š</strong>...</p>
                    </li>
                    </ol>
                </div>
                <div class="summary-section">
                    <h2>è¡¥å……è§‚å¯Ÿ</h2>
                    <p>æš‚æ— æˆ–å…¶ä»–è¡¥å……ã€‚</p>
                </div>
                </div>

                HTML è¦ç”¨ä¸­æ–‡æ’°å†™å†…å®¹ï¼Œé‡ç‚¹æ¨èéƒ¨åˆ†å»ºè®®è¿”å› 3-5 ç¯‡è®ºæ–‡ï¼Œå¯æŒ‰å®é™…æƒ…å†µå¢å‡ï¼Œç¼ºå°‘æ¨èæ—¶è¯·å†™â€œæš‚æ— æ¨èã€‚â€ã€‚
            """

             # -------- English Version --------
        elif self.language == "english":
            prompt_context = """
                You are a helpful academic research assistant.
                Below is the description of my current research field:
                {}
            """.format(self.description)
            papers_context = """
                Here are papers collected from arXiv yesterday.
                Each paper includes its title and summary:
                {}
            """.format(overview)
            json_instruction = """
                Please strictly follow the JSON format below without adding extra text or code blocks:
                {{
                "trend_summary": "<Overall research trend in English, using HTML syntax (no Markdown)>",
                "recommendations": [
                    {{
                    "title": "<Paper title>",
                    "relevance_label": "<Highly relevant / Relevant / Moderately relevant>",
                    "recommend_reason": "<Why this paper is worth reading>",
                    "key_contribution": "<One-sentence key contribution>"
                    }}
                ],
                "additional_observation": "<Additional remarks â€” 'None' if no extra notes>"
                }}

                Task requirements:
                1. Summarize the overall research trend shown in todayâ€™s papers and its relation to my research interest.
                2. Select 3â€“5 most valuable papers and explain why they are recommended.
                3. Provide any additional observations or future directions if relevant.
            """
            html_instruction = """
                Please output a pure HTML snippet following the structure below.
                Do NOT include JSON, Markdown, or any explanation text.
                <div class="summary-wrapper">
                <div class="summary-section">
                    <h2>Today's Research Trend</h2>
                    <p>...</p>
                </div>
                <div class="summary-section">
                    <h2>Top Recommendations</h2>
                    <ol class="summary-list">
                    <li class="summary-item">
                        <div class="summary-item__header">
                        <span class="summary-item__title">Paper title</span>
                        <span class="summary-pill">Relevance</span>
                        </div>
                        <p><strong>Reason:</strong> ...</p>
                        <p><strong>Key contribution:</strong> ...</p>
                    </li>
                    </ol>
                </div>
                <div class="summary-section">
                    <h2>Additional Observations</h2>
                    <p>None</p>
                </div>
                </div>
            """

        # -------- Korean Version (Default) --------
        else:
            prompt_context = """
                ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ í•™ë¬¸ ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ë¡œì„œ,
                ë§¤ì¼ arXiv ë…¼ë¬¸ì„ ë¶„ì„í•˜ê³  ì—°êµ¬ìì—ê²Œ ìœ ìš©í•œ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.
                ì•„ë˜ëŠ” ë‚˜ì˜ ìµœê·¼ ì—°êµ¬ ê´€ì‹¬ ë¶„ì•¼ì…ë‹ˆë‹¤:
                {}
            """.format(self.description)
            papers_context = """
                ì•„ë˜ëŠ” ì–´ì œ arXivì—ì„œ ìˆ˜ì§‘í•œ ë…¼ë¬¸ ëª©ë¡ì…ë‹ˆë‹¤.
                ê° ë…¼ë¬¸ì˜ ì œëª©ê³¼ ìš”ì•½ì´ ì œê³µë©ë‹ˆë‹¤:
                {}
            """.format(overview)
            json_instruction = """
                ì•„ë˜ JSON í˜•ì‹ì„ **ì—„ê²©íˆ** ì§€ì¼œì„œ ì‘ë‹µí•˜ì„¸ìš”.
                ì¶”ê°€ í…ìŠ¤íŠ¸ë‚˜ ì½”ë“œë¸”ë¡, ì£¼ì„ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”:
                {{
                "trend_summary": "<ì „ì²´ì ì¸ ì—°êµ¬ ë™í–¥ â€” í•œêµ­ì–´ë¡œ, HTML ë¬¸ë²• ì‚¬ìš© ê°€ëŠ¥>",
                "recommendations": [
                    {{
                    "title": "<ë…¼ë¬¸ ì œëª©>",
                    "relevance_label": "<ë§¤ìš° ê´€ë ¨ / ê´€ë ¨ / ë³´í†µ ê´€ë ¨>",
                    "recommend_reason": "<ì´ ë…¼ë¬¸ì„ ì¶”ì²œí•˜ëŠ” ì´ìœ >",
                    "key_contribution": "<ë…¼ë¬¸ì˜ í•µì‹¬ ê¸°ì—¬>"
                    }}
                ],
                "additional_observation": "<ì¶”ê°€ ê´€ì°° ë‚´ìš© â€” ì—†ìœ¼ë©´ 'ì—†ìŒ'>"
                }}
            """
            html_instruction = """
                ì•„ë˜ ì˜ˆì‹œ HTML êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¥´ì„¸ìš”.
                JSON, Markdown, ì„¤ëª…ë¬¸ ì—†ì´ **ìˆœìˆ˜ HTML**ë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤:
                <div class="summary-wrapper">
                <div class="summary-section">
                    <h2>ì˜¤ëŠ˜ì˜ ì—°êµ¬ ë™í–¥</h2>
                    <p>...</p>
                </div>
                <div class="summary-section">
                    <h2>ì£¼ìš” ì¶”ì²œ</h2>
                    <ol class="summary-list">
                    <li class="summary-item">
                        <div class="summary-item__header">
                        <span class="summary-item__title">ë…¼ë¬¸ ì œëª©</span>
                        <span class="summary-pill">ê´€ë ¨ë„</span>
                        </div>
                        <p><strong>ì¶”ì²œ ì´ìœ :</strong> ...</p>
                        <p><strong>í•µì‹¬ ê¸°ì—¬:</strong> ...</p>
                    </li>
                    </ol>
                </div>
                <div class="summary-section">
                    <h2>ë³´ì¶© ê´€ì°°</h2>
                    <p>ì—†ìŒ</p>
                </div>
                </div>
            """

        # Common parts shared by all languages
        prompt = prompt_context + papers_context + json_instruction
        html_prompt = prompt_context + papers_context + html_instruction

        def _clean_model_response(raw_text: str) -> str:
            cleaned = raw_text.strip()
            if cleaned.startswith("```"):
                cleaned = cleaned[3:]
                if cleaned.endswith("```"):
                    cleaned = cleaned[:-3]
                cleaned = cleaned.strip()
                if "\n" in cleaned:
                    first_line, rest = cleaned.split("\n", 1)
                    if first_line.strip().lower() in ("json", "html"):
                        cleaned = rest
                    else:
                        cleaned = first_line + "\n" + rest
            return cleaned.strip()

        max_retries = 1
        for attempt in range(1, max_retries + 1):
            try:
                raw_response = self.model.inference(prompt, temperature=self.temperature)
                cleaned = _clean_model_response(raw_response)
                data = json.loads(cleaned)

                if self.language == "english":
                    no_trend = "No trend information available."
                    no_observation = "None."
                    relevance_unknown = "Unknown relevance"
                    no_reason = "No recommendation reason provided."
                    no_contribution = "No key contribution provided."
                    field_error = "The 'recommendations' field must be a list."
                    missing_title = "A recommendation entry is missing its title."
                    summary_fail = "Summary generation failed. Please try again later."
                    html_retry_msg = lambda n: f"HTML fallback attempt {n}..."
                    summary_retry_msg = lambda n, e: f"Summary generation attempt {n} failed: {e}"

                elif self.language == "chinese":
                    no_trend = "æš‚æ— è¶‹åŠ¿ä¿¡æ¯"
                    no_observation = "æš‚æ— "
                    relevance_unknown = "ç›¸å…³æ€§æœªçŸ¥"
                    no_reason = "æœªæä¾›æ¨èç†ç”±"
                    no_contribution = "æœªæä¾›å…³é”®è´¡çŒ®"
                    field_error = "recommendations å­—æ®µä¸æ˜¯åˆ—è¡¨"
                    missing_title = "recommendations ä¸­å­˜åœ¨ç¼ºå°‘æ ‡é¢˜çš„æ¡ç›®"
                    summary_fail = "æ€»ç»“ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚"
                    html_retry_msg = lambda n: f"HTML å›é€€ç”Ÿæˆç¬¬ {n} æ¬¡..."
                    summary_retry_msg = lambda n, e: f"æ€»ç»“ç”Ÿæˆç¬¬ {n} æ¬¡å¤±è´¥: {e}"

                else:  # Default: Korean
                    no_trend = "ì—°êµ¬ ë™í–¥ ì •ë³´ ì—†ìŒ"
                    no_observation = "ì—†ìŒ"
                    relevance_unknown = "ê´€ë ¨ë„ ë¶ˆëª…"
                    no_reason = "ì¶”ì²œ ì´ìœ ê°€ ì—†ìŠµë‹ˆë‹¤."
                    no_contribution = "í•µì‹¬ ê¸°ì—¬ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                    field_error = "recommendations í•„ë“œëŠ” ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤."
                    missing_title = "ì¶”ì²œ í•­ëª©ì— ì œëª©ì´ ì—†ìŠµë‹ˆë‹¤."
                    summary_fail = "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                    html_retry_msg = lambda n: f"HTML ëŒ€ì²´ ìƒì„± ì‹œë„ {n}ë²ˆì§¸..."
                    summary_retry_msg = lambda n, e: f"ìš”ì•½ ìƒì„± {n}ë²ˆì§¸ ì‹¤íŒ¨: {e}"

                trend_summary = data.get("trend_summary", no_trend)
                recommendations_data = data.get("recommendations", [])
                additional_observation = data.get("additional_observation", no_observation)

                if not isinstance(recommendations_data, list):
                    raise ValueError(field_error)

                cleaned_recommendations = []
                for item in recommendations_data:
                    title = item.get("title")
                    if not title:
                        raise ValueError(missing_title)
                    cleaned_recommendations.append(
                        {
                            "title": title,
                            "relevance_label": item.get("relevance_label", relevance_unknown),
                            "recommend_reason": item.get("recommend_reason", no_reason),
                            "key_contribution": item.get("key_contribution", no_contribution),
                        }
                    )

                structured_summary = {
                    "trend_summary": trend_summary,
                    "recommendations": cleaned_recommendations,
                    "additional_observation": additional_observation,
                }

                return render_summary_sections(structured_summary)

            except Exception as error:
                print(summary_retry_msg(attempt, error))
                if attempt == max_retries:
                    try:
                        for html_attempt in range(1, max_retries + 1):
                            print(html_retry_msg(html_attempt))
                            raw_html_response = self.model.inference(html_prompt, temperature=self.temperature)
                            cleaned_html = _clean_model_response(raw_html_response)
                            return cleaned_html
                    except Exception as html_error:
                        print(f"[HTML Fallback Error] {html_error}")
                        fallback_data = {
                            "trend_summary": summary_fail,
                            "recommendations": [],
                            "additional_observation": no_observation,
                        }
                        return render_summary_sections(fallback_data)



    def render_email(self, recommendations):
        # Set the path for saving the rendered HTML email
        save_file_path = os.path.join(self.save_dir, self.run_date, "arxiv_daily_email.html")

        # Load from cache if the email has already been rendered
        if os.path.exists(save_file_path):
            with open(save_file_path, "r", encoding="utf-8") as f:
                print(f"Email already rendered. Loading from cache file: {save_file_path}")
                return f.read()

        parts = []

        # Return empty HTML if no recommendations exist
        if len(recommendations) == 0:
            return framework.replace("__CONTENT__", get_empty_html())

        # Convert each paper's information into an HTML block
        for i, p in enumerate(tqdm(recommendations, desc="Rendering email")):
            rate = get_stars(p["relevance_score"])
            parts.append(
                get_block_html(
                    str(i + 1) + ". " + p["title"],  # title
                    rate,                            # star rating
                    p["arXiv_id"],                   # arXiv ID
                    p["summary"],                    # summary
                    p["pdf_url"],                    # PDF link
                )
            )

        # Add overall summary to the top of the email
        summary = self.summarize(recommendations)
        content = summary
        content += "<br>" + "</br><br>".join(parts) + "</br>"

        # Insert content into the HTML framework
        email_html = framework.replace("__CONTENT__", content)

        # Save the rendered email to the specified directory
        if self.save_dir:
            base_dir = os.path.dirname(os.path.abspath(__file__))
            save_path = os.path.join(base_dir, self.save_dir, self.run_date, "arxiv_daily_email.html")
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            with open(save_path, "w", encoding="utf-8") as f:
                f.write(email_html)

        return email_html


    def send_email(
        self,
        sender: str,
        receiver: str,
        password: str,
        smtp_server: str,
        smtp_port: int,
        title: str,
    ):
        recommendations = self.get_recommendation()
        html = self.render_email(recommendations)

        def _format_addr(s):
            name, addr = parseaddr(s)
            return formataddr((Header(name, "utf-8").encode(), addr))

        msg = MIMEText(html, "html", "utf-8")
        msg["From"] = _format_addr(f"{title} <%s>" % sender)

        receivers = [addr.strip() for addr in receiver.split(",")]
        print(receivers)
        msg["To"] = ",".join([_format_addr(f"You <%s>" % addr) for addr in receivers])

        today = self.run_datetime.strftime("%Y/%m/%d")
        msg["Subject"] = Header(f"{title} {today}", "utf-8").encode()

        try:
            server = smtplib.SMTP(smtp_server, smtp_port)
            server.starttls()
        except Exception as e:
            logger.warning(f"Failed to use TLS. {e}")
            logger.warning(f"Try to use SSL.")
            server = smtplib.SMTP_SSL(smtp_server, smtp_port)

        server.login(sender, password)
        server.sendmail(sender, receivers, msg.as_string())
        server.quit()


    def send_slack(self, webhook_url: str, title: str):
        recommendations = self.get_recommendation()

        self.render_email(recommendations)

        # Select the top 3 papers based on relevance_score
        top_recommendations = sorted(
            recommendations, key=lambda x: x["relevance_score"], reverse=True
        )[:3]

        # Build Slack message blocks (Markdown-based formatting)
        message_blocks = [
            {
                "type": "header",
                "text": {"type": "plain_text", "text": f"ğŸ“š {title} ({self.run_date})"}
            },
            {
                "type": "section",
                "text": {"type": "mrkdwn", "text": "*Here are todayâ€™s Top 3 paper summaries.*"}
            },
            {"type": "divider"}
        ]

        for i, paper in enumerate(top_recommendations, start=1):
            text = (
                f"*{i}. {paper['title']}*\n"
                f"{paper['summary'][:500]}...\n"
                f"<{paper['pdf_url']}|[PDF Link]>  Â·  Relevance: {paper['relevance_score']:.1f}"
            )
            message_blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": text}})
            message_blocks.append({"type": "divider"})

        # Send message payload to Slack webhook
        payload = {"blocks": message_blocks}
        response = requests.post(webhook_url, json=payload)

        if response.status_code == 200:
            print("âœ… Successfully sent Slack message (Top 3 papers).")
        else:
            print(f"âš ï¸ Failed to send Slack message: {response.status_code}, {response.text}")



if __name__ == "__main__":
    categories = ["cs.CV"]
    max_entries = 100
    max_paper_num = 50
    provider = "ollama"
    model = "deepseek-r1:7b"
    description = """
        My research focuses on building intelligent speech systems that combine language understanding and generation. 
        I am particularly interested in how speech, prosody, and emotion can be modeled together with language to create more natural and human-like AI agents. 
        My work explores generative and multimodal approaches that unify speech, text, and visual information â€” enabling AI systems that can not only speak but also understand context, emotion, and intent in conversation.
    """
    

    arxiv_daily = ArxivDaily(
        categories, max_entries, max_paper_num, provider, model, None, None, description
    )
    recommendations = arxiv_daily.get_recommendation()
    print(recommendations)
